{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Update the weights to reduce loss\n",
    "\n",
    "#### w1 = w1 - *learning rate* * d/dw1\n",
    "\n",
    "learning rate is about 0.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   age  affordibility  bought_insurance\n0   22              1                 0\n1   25              0                 0\n2   47              1                 1\n3   52              0                 0\n4   46              1                 1\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(28, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(f\"{os.path.dirname(os.path.abspath('__file__'))}\\\\4_insurance_data.csv\")\n",
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     age  affordibility\n",
       "2   0.47              1\n",
       "10  0.18              1\n",
       "21  0.26              0\n",
       "11  0.28              1\n",
       "14  0.49              1\n",
       "9   0.61              1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>affordibility</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>0.47</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.18</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.26</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.28</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.49</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.61</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# split train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['age','affordibility']],df.bought_insurance,test_size=0.2, random_state=25)\n",
    "\n",
    "# apply scaling\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled['age'] = X_train_scaled['age'] / 100\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled['age'] = X_test_scaled['age'] / 100\n",
    "\n",
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n",
      "Epoch 4803/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4669 - accuracy: 0.9091\n",
      "Epoch 4804/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4669 - accuracy: 0.9091\n",
      "Epoch 4805/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4669 - accuracy: 0.9091\n",
      "Epoch 4806/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4668 - accuracy: 0.9091\n",
      "Epoch 4807/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4668 - accuracy: 0.9091\n",
      "Epoch 4808/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4668 - accuracy: 0.9091\n",
      "Epoch 4809/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4668 - accuracy: 0.9091\n",
      "Epoch 4810/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4668 - accuracy: 0.9091\n",
      "Epoch 4811/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.9091\n",
      "Epoch 4812/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.9091\n",
      "Epoch 4813/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.9091\n",
      "Epoch 4814/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.9091\n",
      "Epoch 4815/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.9091\n",
      "Epoch 4816/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4666 - accuracy: 0.9091\n",
      "Epoch 4817/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4666 - accuracy: 0.9091\n",
      "Epoch 4818/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4666 - accuracy: 0.9091\n",
      "Epoch 4819/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4666 - accuracy: 0.9091\n",
      "Epoch 4820/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.9091\n",
      "Epoch 4821/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.9091\n",
      "Epoch 4822/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.9091\n",
      "Epoch 4823/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.9091\n",
      "Epoch 4824/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.9091\n",
      "Epoch 4825/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.9091\n",
      "Epoch 4826/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.9091\n",
      "Epoch 4827/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.9091\n",
      "Epoch 4828/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.9091\n",
      "Epoch 4829/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.9091\n",
      "Epoch 4830/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.9091\n",
      "Epoch 4831/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4663 - accuracy: 0.9091\n",
      "Epoch 4832/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4663 - accuracy: 0.9091\n",
      "Epoch 4833/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4663 - accuracy: 0.9091\n",
      "Epoch 4834/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.9091\n",
      "Epoch 4835/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4663 - accuracy: 0.9091\n",
      "Epoch 4836/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4662 - accuracy: 0.9091\n",
      "Epoch 4837/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4662 - accuracy: 0.9091\n",
      "Epoch 4838/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4662 - accuracy: 0.9091\n",
      "Epoch 4839/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4662 - accuracy: 0.9091\n",
      "Epoch 4840/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4662 - accuracy: 0.9091\n",
      "Epoch 4841/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4661 - accuracy: 0.9091\n",
      "Epoch 4842/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4661 - accuracy: 0.9091\n",
      "Epoch 4843/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4661 - accuracy: 0.9091\n",
      "Epoch 4844/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4661 - accuracy: 0.9091\n",
      "Epoch 4845/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4661 - accuracy: 0.9091\n",
      "Epoch 4846/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4660 - accuracy: 0.9091\n",
      "Epoch 4847/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.9091\n",
      "Epoch 4848/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.9091\n",
      "Epoch 4849/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.9091\n",
      "Epoch 4850/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.9091\n",
      "Epoch 4851/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4852/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4853/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4854/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4855/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4856/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.9091\n",
      "Epoch 4857/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4658 - accuracy: 0.9091\n",
      "Epoch 4858/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4658 - accuracy: 0.9091\n",
      "Epoch 4859/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4658 - accuracy: 0.9091\n",
      "Epoch 4860/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.9091\n",
      "Epoch 4861/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.9091\n",
      "Epoch 4862/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.9091\n",
      "Epoch 4863/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4657 - accuracy: 0.9091\n",
      "Epoch 4864/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4657 - accuracy: 0.9091\n",
      "Epoch 4865/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.9091\n",
      "Epoch 4866/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4657 - accuracy: 0.9091\n",
      "Epoch 4867/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4656 - accuracy: 0.9091\n",
      "Epoch 4868/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4656 - accuracy: 0.9091\n",
      "Epoch 4869/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4656 - accuracy: 0.9091\n",
      "Epoch 4870/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4656 - accuracy: 0.9091\n",
      "Epoch 4871/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4656 - accuracy: 0.9091\n",
      "Epoch 4872/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4655 - accuracy: 0.9091\n",
      "Epoch 4873/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4655 - accuracy: 0.9091\n",
      "Epoch 4874/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4655 - accuracy: 0.9091\n",
      "Epoch 4875/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4655 - accuracy: 0.9091\n",
      "Epoch 4876/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4655 - accuracy: 0.9091\n",
      "Epoch 4877/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.9091\n",
      "Epoch 4878/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.9091\n",
      "Epoch 4879/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.9091\n",
      "Epoch 4880/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.9091\n",
      "Epoch 4881/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.9091\n",
      "Epoch 4882/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4653 - accuracy: 0.9091\n",
      "Epoch 4883/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4653 - accuracy: 0.9091\n",
      "Epoch 4884/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4653 - accuracy: 0.9091\n",
      "Epoch 4885/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4653 - accuracy: 0.9091\n",
      "Epoch 4886/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4653 - accuracy: 0.9091\n",
      "Epoch 4887/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4652 - accuracy: 0.9091\n",
      "Epoch 4888/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4652 - accuracy: 0.9091\n",
      "Epoch 4889/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4652 - accuracy: 0.9091\n",
      "Epoch 4890/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4652 - accuracy: 0.9091\n",
      "Epoch 4891/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4652 - accuracy: 0.9091\n",
      "Epoch 4892/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.9091\n",
      "Epoch 4893/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.9091\n",
      "Epoch 4894/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.9091\n",
      "Epoch 4895/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.9091\n",
      "Epoch 4896/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.9091\n",
      "Epoch 4897/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4898/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4899/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4900/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4901/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4902/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4650 - accuracy: 0.9091\n",
      "Epoch 4903/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.9091\n",
      "Epoch 4904/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4649 - accuracy: 0.9091\n",
      "Epoch 4905/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.9091\n",
      "Epoch 4906/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.9091\n",
      "Epoch 4907/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.9091\n",
      "Epoch 4908/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4648 - accuracy: 0.9091\n",
      "Epoch 4909/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4648 - accuracy: 0.9091\n",
      "Epoch 4910/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4648 - accuracy: 0.9091\n",
      "Epoch 4911/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4648 - accuracy: 0.9091\n",
      "Epoch 4912/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.9091\n",
      "Epoch 4913/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4647 - accuracy: 0.9091\n",
      "Epoch 4914/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4647 - accuracy: 0.9091\n",
      "Epoch 4915/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4647 - accuracy: 0.9091\n",
      "Epoch 4916/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4647 - accuracy: 0.9091\n",
      "Epoch 4917/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4647 - accuracy: 0.9091\n",
      "Epoch 4918/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.9091\n",
      "Epoch 4919/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.9091\n",
      "Epoch 4920/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.9091\n",
      "Epoch 4921/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.9091\n",
      "Epoch 4922/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.9091\n",
      "Epoch 4923/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4645 - accuracy: 0.9091\n",
      "Epoch 4924/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4645 - accuracy: 0.9091\n",
      "Epoch 4925/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4645 - accuracy: 0.9091\n",
      "Epoch 4926/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4645 - accuracy: 0.9091\n",
      "Epoch 4927/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4645 - accuracy: 0.9091\n",
      "Epoch 4928/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4929/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4930/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4931/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4932/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4933/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.9091\n",
      "Epoch 4934/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4643 - accuracy: 0.9091\n",
      "Epoch 4935/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4643 - accuracy: 0.9091\n",
      "Epoch 4936/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4643 - accuracy: 0.9091\n",
      "Epoch 4937/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4643 - accuracy: 0.9091\n",
      "Epoch 4938/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4643 - accuracy: 0.9091\n",
      "Epoch 4939/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4642 - accuracy: 0.9091\n",
      "Epoch 4940/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4642 - accuracy: 0.9091\n",
      "Epoch 4941/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4642 - accuracy: 0.9091\n",
      "Epoch 4942/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4642 - accuracy: 0.9091\n",
      "Epoch 4943/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4642 - accuracy: 0.9091\n",
      "Epoch 4944/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.9091\n",
      "Epoch 4945/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.9091\n",
      "Epoch 4946/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.9091\n",
      "Epoch 4947/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.9091\n",
      "Epoch 4948/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.9091\n",
      "Epoch 4949/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.9091\n",
      "Epoch 4950/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4640 - accuracy: 0.9091\n",
      "Epoch 4951/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.9091\n",
      "Epoch 4952/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4640 - accuracy: 0.9091\n",
      "Epoch 4953/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4640 - accuracy: 0.9091\n",
      "Epoch 4954/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4955/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4956/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4957/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4958/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4959/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.9091\n",
      "Epoch 4960/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.9091\n",
      "Epoch 4961/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.9091\n",
      "Epoch 4962/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.9091\n",
      "Epoch 4963/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.9091\n",
      "Epoch 4964/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.9091\n",
      "Epoch 4965/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.9091\n",
      "Epoch 4966/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4637 - accuracy: 0.9091\n",
      "Epoch 4967/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4637 - accuracy: 0.9091\n",
      "Epoch 4968/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4637 - accuracy: 0.9091\n",
      "Epoch 4969/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4637 - accuracy: 0.9091\n",
      "Epoch 4970/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.9091\n",
      "Epoch 4971/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.9091\n",
      "Epoch 4972/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.9091\n",
      "Epoch 4973/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4636 - accuracy: 0.9091\n",
      "Epoch 4974/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.9091\n",
      "Epoch 4975/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4976/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4977/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4978/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4979/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4980/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4635 - accuracy: 0.9091\n",
      "Epoch 4981/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4634 - accuracy: 0.9091\n",
      "Epoch 4982/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.9091\n",
      "Epoch 4983/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.9091\n",
      "Epoch 4984/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4634 - accuracy: 0.9091\n",
      "Epoch 4985/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4634 - accuracy: 0.9091\n",
      "Epoch 4986/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.9091\n",
      "Epoch 4987/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.9091\n",
      "Epoch 4988/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4633 - accuracy: 0.9091\n",
      "Epoch 4989/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4633 - accuracy: 0.9091\n",
      "Epoch 4990/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4633 - accuracy: 0.9091\n",
      "Epoch 4991/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4632 - accuracy: 0.9091\n",
      "Epoch 4992/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4632 - accuracy: 0.9091\n",
      "Epoch 4993/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4632 - accuracy: 0.9091\n",
      "Epoch 4994/5000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.9091\n",
      "Epoch 4995/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4632 - accuracy: 0.9091\n",
      "Epoch 4996/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4631 - accuracy: 0.9091\n",
      "Epoch 4997/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4631 - accuracy: 0.9091\n",
      "Epoch 4998/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4631 - accuracy: 0.9091\n",
      "Epoch 4999/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4631 - accuracy: 0.9091\n",
      "Epoch 5000/5000\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4631 - accuracy: 0.9091\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c20faf2040>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, input_shape=(2,), activation='sigmoid', kernel_initializer='ones', bias_initializer='zeros')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3550 - accuracy: 1.0000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.3549775183200836, 1.0]"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test) # perfect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[5.0608673],\n",
       "        [1.4086505]], dtype=float32),\n",
       " array([-2.9137032], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "coef, intercept = model.get_weights()\n",
    "coef, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# manual prediction funcion using sigmoid\n",
    "def prediction_function(age, affordibility):\n",
    "    return sigmoid(coef[0] * age + coef[1] * affordibility + intercept)\n",
    "\n",
    "def log_loss(y_actual, y_predicted):\n",
    "    eplison = 1e-15\n",
    "    y_predicted_new = np.array([min(max(i, eplison), 1 - eplison) for i in y_predicted])\n",
    "\n",
    "    return -np.mean(y_actual * np.log(y_predicted_new) + (1 - y_actual) * np.log(1 - y_predicted_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.70548487]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.70548487],\n",
       "       [0.35569552],\n",
       "       [0.16827844],\n",
       "       [0.47801176],\n",
       "       [0.7260696 ],\n",
       "       [0.82949835]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "print(prediction_function(0.47, 1))\n",
    "print(model.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.4047108446120309 - bias: -2.2621692830941247 - loss: 0.5001933919340792)\n",
      "Epoch: 202/1000 - w1: 3.4656079374887283 - w2: 1.4051944805878065 - bias: -2.2671884898023182 - loss: 0.4998869137205552)\n",
      "Epoch: 203/1000 - w1: 3.4768709612509157 - w2: 1.4056728830805474 - bias: -2.2721947079716065 - loss: 0.499581686073539)\n",
      "Epoch: 204/1000 - w1: 3.4881123852216054 - w2: 1.4061461626631908 - bias: -2.2771880355996448 - loss: 0.4992777027855931)\n",
      "Epoch: 205/1000 - w1: 3.4993322397861726 - w2: 1.406614427814455 - bias: -2.282168569236039 - loss: 0.4989749576960279)\n",
      "Epoch: 206/1000 - w1: 3.510530555686274 - w2: 1.407077784957646 - bias: -2.287136404007907 - loss: 0.4986734446900829)\n",
      "Epoch: 207/1000 - w1: 3.5217073640113545 - w2: 1.407536338498725 - bias: -2.2920916336449575 - loss: 0.49837315769813556)\n",
      "Epoch: 208/1000 - w1: 3.5328626961903016 - w2: 1.4079901908636536 - bias: -2.2970343505041027 - loss: 0.49807409069493364)\n",
      "Epoch: 209/1000 - w1: 3.5439965839832466 - w2: 1.4084394425350295 - bias: -2.30196464559361 - loss: 0.49777623769885115)\n",
      "Epoch: 210/1000 - w1: 3.5551090594735104 - w2: 1.4088841920880264 - bias: -2.3068826085968004 - loss: 0.49747959277116743)\n",
      "Epoch: 211/1000 - w1: 3.5662001550596885 - w2: 1.4093245362256548 - bias: -2.3117883278953073 - loss: 0.4971841500153676)\n",
      "Epoch: 212/1000 - w1: 3.577269903447877 - w2: 1.4097605698133546 - bias: -2.3166818905919 - loss: 0.4968899035764641)\n",
      "Epoch: 213/1000 - w1: 3.5883183376440364 - w2: 1.4101923859129344 - bias: -2.321563382532884 - loss: 0.49659684764033973)\n",
      "Epoch: 214/1000 - w1: 3.5993454909464844 - w2: 1.4106200758158696 - bias: -2.32643288833008 - loss: 0.49630497643310856)\n",
      "Epoch: 215/1000 - w1: 3.610351396938527 - w2: 1.4110437290759732 - bias: -2.331290491382403 - loss: 0.49601428422049737)\n",
      "Epoch: 216/1000 - w1: 3.6213360894812157 - w2: 1.4114634335414504 - bias: -2.3361362738970306 - loss: 0.4957247653072439)\n",
      "Epoch: 217/1000 - w1: 3.6322996027062318 - w2: 1.4118792753863514 - bias: -2.3409703169101865 - loss: 0.49543641403651506)\n",
      "Epoch: 218/1000 - w1: 3.6432419710088983 - w2: 1.4122913391414305 - bias: -2.345792700307536 - loss: 0.49514922478933926)\n",
      "Epoch: 219/1000 - w1: 3.6541632290413135 - w2: 1.4126997077244283 - bias: -2.350603502844203 - loss: 0.4948631919840572)\n",
      "Epoch: 220/1000 - w1: 3.6650634117056042 - w2: 1.4131044624697842 - bias: -2.3554028021644196 - loss: 0.49457831007578845)\n",
      "Epoch: 221/1000 - w1: 3.6759425541473023 - w2: 1.4135056831577921 - bias: -2.360190674820809 - loss: 0.4942945735559119)\n",
      "Epoch: 222/1000 - w1: 3.6868006917488327 - w2: 1.4139034480432113 - bias: -2.3649671962933168 - loss: 0.49401197695156235)\n",
      "Epoch: 223/1000 - w1: 3.6976378601231215 - w2: 1.4142978338833405 - bias: -2.3697324410077902 - loss: 0.49373051482514135)\n",
      "Epoch: 224/1000 - w1: 3.7084540951073124 - w2: 1.4146889159655687 - bias: -2.3744864823542158 - loss: 0.49345018177384004)\n",
      "Epoch: 225/1000 - w1: 3.719249432756598 - w2: 1.4150767681344103 - bias: -2.379229392704622 - loss: 0.49317097242917785)\n",
      "Epoch: 226/1000 - w1: 3.7300239093381586 - w2: 1.4154614628180373 - bias: -2.383961243430653 - loss: 0.4928928814565522)\n",
      "Epoch: 227/1000 - w1: 3.740777561325208 - w2: 1.4158430710543157 - bias: -2.388682104920817 - loss: 0.4926159035548014)\n",
      "Epoch: 228/1000 - w1: 3.751510425391147 - w2: 1.4162216625163582 - bias: -2.393392046597421 - loss: 0.4923400334557782)\n",
      "Epoch: 229/1000 - w1: 3.7622225384038157 - w2: 1.4165973055376 - bias: -2.398091136933194 - loss: 0.4920652659239369)\n",
      "Epoch: 230/1000 - w1: 3.772913937419856 - w2: 1.4169700671364103 - bias: -2.4027794434676064 - loss: 0.4917915957559304)\n",
      "Epoch: 231/1000 - w1: 3.783584659679165 - w2: 1.417340013040245 - bias: -2.407457032822888 - loss: 0.49151901778021734)\n",
      "Epoch: 232/1000 - w1: 3.7942347425994543 - w2: 1.4177072077093518 - bias: -2.412123970719756 - loss: 0.4912475268566808)\n",
      "Epoch: 233/1000 - w1: 3.804864223770903 - w2: 1.4180717143600363 - bias: -2.4167803219928548 - loss: 0.4909771178762567)\n",
      "Epoch: 234/1000 - w1: 3.8154731409509055 - w2: 1.418433594987496 - bias: -2.4214261506059125 - loss: 0.4907077857605718)\n",
      "Epoch: 235/1000 - w1: 3.8260615320589157 - w2: 1.4187929103882317 - bias: -2.426061519666623 - loss: 0.49043952546159064)\n",
      "Epoch: 236/1000 - w1: 3.83662943517138 - w2: 1.4191497201820455 - bias: -2.4306864914412545 - loss: 0.4901723319612717)\n",
      "Epoch: 237/1000 - w1: 3.8471768885167634 - w2: 1.41950408283363 - bias: -2.4353011273689953 - loss: 0.4899062002712329)\n",
      "Epoch: 238/1000 - w1: 3.857703930470663 - w2: 1.4198560556737607 - bias: -2.4399054880760342 - loss: 0.48964112543242444)\n",
      "Epoch: 239/1000 - w1: 3.8682105995510105 - w2: 1.4202056949200956 - bias: -2.444499633389389 - loss: 0.48937710251481087)\n",
      "Epoch: 240/1000 - w1: 3.8786969344133597 - w2: 1.4205530556975927 - bias: -2.4490836223504795 - loss: 0.48911412661705983)\n",
      "Epoch: 241/1000 - w1: 3.8891629738462594 - w2: 1.4208981920585493 - bias: -2.45365751322846 - loss: 0.48885219286623943)\n",
      "Epoch: 242/1000 - w1: 3.8996087567667086 - w2: 1.4212411570022738 - bias: -2.4582213635333026 - loss: 0.48859129641752214)\n",
      "Epoch: 243/1000 - w1: 3.910034322215694 - w2: 1.4215820024943946 - bias: -2.4627752300286496 - loss: 0.48833143245389626)\n",
      "Epoch: 244/1000 - w1: 3.920439709353808 - w2: 1.4219207794858135 - bias: -2.4673191687444302 - loss: 0.48807259618588394)\n",
      "Epoch: 245/1000 - w1: 3.930824957456947 - w2: 1.4222575379313118 - bias: -2.471853234989249 - loss: 0.48781478285126634)\n",
      "Epoch: 246/1000 - w1: 3.9411901059120837 - w2: 1.422592326807813 - bias: -2.4763774833625503 - loss: 0.4875579877148143)\n",
      "Epoch: 247/1000 - w1: 3.9515351942131214 - w2: 1.422925194132311 - bias: -2.4808919677665644 - loss: 0.4873022060680255)\n",
      "Epoch: 248/1000 - w1: 3.9618602619568177 - w2: 1.4232561869794709 - bias: -2.485396741418036 - loss: 0.487047433228868)\n",
      "Epoch: 249/1000 - w1: 3.972165348838788 - w2: 1.4235853514989025 - bias: -2.489891856859742 - loss: 0.4867936645415293)\n",
      "Epoch: 250/1000 - w1: 3.982450494649576 - w2: 1.4239127329321233 - bias: -2.494377365971801 - loss: 0.48654089537617085)\n",
      "Epoch: 251/1000 - w1: 3.9927157392707997 - w2: 1.4242383756292052 - bias: -2.49885331998278 - loss: 0.48628912112868766)\n",
      "Epoch: 252/1000 - w1: 4.002961122671366 - w2: 1.42456232306512 - bias: -2.503319769480601 - loss: 0.48603833722047357)\n",
      "Epoch: 253/1000 - w1: 4.013186684903756 - w2: 1.4248846178557855 - bias: -2.50777676442325 - loss: 0.48578853909819186)\n",
      "Epoch: 254/1000 - w1: 4.023392466100375 - w2: 1.4252053017738153 - bias: -2.512224354149295 - loss: 0.4855397222335499)\n",
      "Epoch: 255/1000 - w1: 4.033578506469971 - w2: 1.425524415763985 - bias: -2.516662587388215 - loss: 0.4852918821230787)\n",
      "Epoch: 256/1000 - w1: 4.043744846294124 - w2: 1.4258419999584127 - bias: -2.5210915122705426 - loss: 0.4850450142879177)\n",
      "Epoch: 257/1000 - w1: 4.053891525923789 - w2: 1.4261580936914633 - bias: -2.525511176337824 - loss: 0.48479911427360317)\n",
      "Epoch: 258/1000 - w1: 4.064018585775913 - w2: 1.4264727355143816 - bias: -2.529921626552404 - loss: 0.48455417764986114)\n",
      "Epoch: 259/1000 - w1: 4.074126066330109 - w2: 1.4267859632096573 - bias: -2.534322909307031 - loss: 0.48431020001040465)\n",
      "Epoch: 260/1000 - w1: 4.084214008125393 - w2: 1.4270978138051291 - bias: -2.5387150704342933 - loss: 0.48406717697273477)\n",
      "Epoch: 261/1000 - w1: 4.094282451756979 - w2: 1.4274083235878308 - bias: -2.5430981552158864 - loss: 0.4838251041779449)\n",
      "Epoch: 262/1000 - w1: 4.104331437873139 - w2: 1.4277175281175865 - bias: -2.5474722083917123 - loss: 0.48358397729053065)\n",
      "Epoch: 263/1000 - w1: 4.114361007172114 - w2: 1.428025462240357 - bias: -2.5518372741688182 - loss: 0.4833437919982009)\n",
      "Epoch: 264/1000 - w1: 4.124371200399091 - w2: 1.4283321601013441 - bias: -2.5561933962301766 - loss: 0.4831045440116944)\n",
      "Epoch: 265/1000 - w1: 4.134362058343228 - w2: 1.4286376551578572 - bias: -2.5605406177433045 - loss: 0.48286622906459864)\n",
      "Epoch: 266/1000 - w1: 4.144333621834739 - w2: 1.4289419801919439 - bias: -2.564878981368735 - loss: 0.48262884291317154)\n",
      "Epoch: 267/1000 - w1: 4.1542859317420335 - w2: 1.429245167322793 - bias: -2.569208529268332 - loss: 0.48239238133616846)\n",
      "Epoch: 268/1000 - w1: 4.164219028968911 - w2: 1.4295472480189122 - bias: -2.573529303113461 - loss: 0.48215684013466936)\n",
      "Epoch: 269/1000 - w1: 4.174132954451802 - w2: 1.4298482531100827 - bias: -2.577841344093013 - loss: 0.481922215131912)\n",
      "Epoch: 270/1000 - w1: 4.184027749157071 - w2: 1.430148212799099 - bias: -2.5821446929212852 - loss: 0.4816885021731251)\n",
      "Epoch: 271/1000 - w1: 4.1939034540783595 - w2: 1.4304471566732948 - bias: -2.5864393898457227 - loss: 0.4814556971253665)\n",
      "Epoch: 272/1000 - w1: 4.203760110233989 - w2: 1.4307451137158596 - bias: -2.5907254746545223 - loss: 0.4812237958773629)\n",
      "Epoch: 273/1000 - w1: 4.213597758664405 - w2: 1.4310421123169506 - bias: -2.5950029866841016 - loss: 0.480992794339352)\n",
      "Epoch: 274/1000 - w1: 4.223416440429678 - w2: 1.4313381802846028 - bias: -2.5992719648264337 - loss: 0.48076268844292946)\n",
      "Epoch: 275/1000 - w1: 4.233216196607044 - w2: 1.4316333448554421 - bias: -2.6035324475362565 - loss: 0.48053347414089403)\n",
      "Epoch: 276/1000 - w1: 4.242997068288498 - w2: 1.4319276327052033 - bias: -2.607784472838149 - loss: 0.48030514740709995)\n",
      "Epoch: 277/1000 - w1: 4.252759096578431 - w2: 1.4322210699590585 - bias: -2.612028078333486 - loss: 0.4800777042363081)\n",
      "Epoch: 278/1000 - w1: 4.262502322591309 - w2: 1.4325136822017583 - bias: -2.616263301207267 - loss: 0.4798511406440404)\n",
      "Epoch: 279/1000 - w1: 4.272226787449408 - w2: 1.4328054944875899 - bias: -2.620490178234828 - loss: 0.47962545266643786)\n",
      "Epoch: 280/1000 - w1: 4.281932532280578 - w2: 1.433096531350154 - bias: -2.6247087457884315 - loss: 0.4794006363601184)\n",
      "Epoch: 281/1000 - w1: 4.291619598216061 - w2: 1.4333868168119661 - bias: -2.628919039843741 - loss: 0.4791766878020384)\n",
      "Epoch: 282/1000 - w1: 4.301288026388348 - w2: 1.4336763743938834 - bias: -2.633121095986182 - loss: 0.4789536030893553)\n",
      "Epoch: 283/1000 - w1: 4.3109378579290745 - w2: 1.4339652271243617 - bias: -2.637314949417191 - loss: 0.478731378339293)\n",
      "Epoch: 284/1000 - w1: 4.320569133966964 - w2: 1.4342533975485452 - bias: -2.6415006349603525 - loss: 0.47851000968900853)\n",
      "Epoch: 285/1000 - w1: 4.330181895625803 - w2: 1.4345409077371913 - bias: -2.6456781870674306 - loss: 0.4782894932954603)\n",
      "Epoch: 286/1000 - w1: 4.339776184022467 - w2: 1.4348277792954356 - bias: -2.6498476398242916 - loss: 0.47806982533527886)\n",
      "Epoch: 287/1000 - w1: 4.349352040264972 - w2: 1.435114033371397 - bias: -2.6540090269567256 - loss: 0.47785100200463954)\n",
      "Epoch: 288/1000 - w1: 4.358909505450577 - w2: 1.4353996906646287 - bias: -2.658162381836163 - loss: 0.4776330195191349)\n",
      "Epoch: 289/1000 - w1: 4.368448620663914 - w2: 1.4356847714344168 - bias: -2.6623077374852917 - loss: 0.47741587411365194)\n",
      "Epoch: 290/1000 - w1: 4.377969426975163 - w2: 1.4359692955079275 - bias: -2.666445126583577 - loss: 0.4771995620422475)\n",
      "Epoch: 291/1000 - w1: 4.387471965438257 - w2: 1.43625328228821 - bias: -2.6705745814726813 - loss: 0.47698407957802763)\n",
      "Epoch: 292/1000 - w1: 4.396956277089129 - w2: 1.436536750762052 - bias: -2.674696134161793 - loss: 0.476769423013028)\n",
      "Epoch: 293/1000 - w1: 4.406422402943988 - w2: 1.4368197195076962 - bias: -2.6788098163328593 - loss: 0.47655558865809444)\n",
      "Epoch: 294/1000 - w1: 4.415870383997634 - w2: 1.4371022067024166 - bias: -2.682915659345727 - loss: 0.4763425728427668)\n",
      "Epoch: 295/1000 - w1: 4.425300261221806 - w2: 1.437384230129958 - bias: -2.6870136942431952 - loss: 0.476130371915163)\n",
      "Epoch: 296/1000 - w1: 4.434712075563564 - w2: 1.4376658071878416 - bias: -2.69110395175598 - loss: 0.47591898224186396)\n",
      "Epoch: 297/1000 - w1: 4.4441058679436996 - w2: 1.4379469548945394 - bias: -2.6951864623075887 - loss: 0.4757084002078021)\n",
      "Epoch: 298/1000 - w1: 4.453481679255187 - w2: 1.4382276898965187 - bias: -2.6992612560191143 - loss: 0.47549862221614797)\n",
      "Epoch: 299/1000 - w1: 4.462839550361659 - w2: 1.4385080284751603 - bias: -2.703328362713941 - loss: 0.4752896446882009)\n",
      "Epoch: 300/1000 - w1: 4.472179522095915 - w2: 1.438787986553552 - bias: -2.707387811922373 - loss: 0.4750814640632793)\n",
      "Epoch: 301/1000 - w1: 4.4815016352584625 - w2: 1.439067579703159 - bias: -2.711439632886177 - loss: 0.47487407679861204)\n",
      "Epoch: 302/1000 - w1: 4.49080593061609 - w2: 1.4393468231503754 - bias: -2.7154838545630504 - loss: 0.47466747936923254)\n",
      "Epoch: 303/1000 - w1: 4.500092448900464 - w2: 1.4396257317829577 - bias: -2.7195205056310083 - loss: 0.47446166826787173)\n",
      "Epoch: 304/1000 - w1: 4.5093612308067605 - w2: 1.4399043201563404 - bias: -2.723549614492697 - loss: 0.4742566400048545)\n",
      "Epoch: 305/1000 - w1: 4.518612316992322 - w2: 1.440182602499841 - bias: -2.72757120927963 - loss: 0.4740523911079949)\n",
      "Epoch: 306/1000 - w1: 4.527845748075346 - w2: 1.4404605927227512 - bias: -2.731585317856352 - loss: 0.4738489181224942)\n",
      "Epoch: 307/1000 - w1: 4.537061564633595 - w2: 1.440738304420319 - bias: -2.7355919678245306 - loss: 0.47364621761083875)\n",
      "Epoch: 308/1000 - w1: 4.546259807203141 - w2: 1.441015750879624 - bias: -2.7395911865269764 - loss: 0.47344428615270023)\n",
      "Epoch: 309/1000 - w1: 4.555440516277136 - w2: 1.4412929450853456 - bias: -2.7435830010515936 - loss: 0.4732431203448344)\n",
      "Epoch: 310/1000 - w1: 4.564603732304602 - w2: 1.4415698997254285 - bias: -2.747567438235262 - loss: 0.473042716800984)\n",
      "Epoch: 311/1000 - w1: 4.573749495689255 - w2: 1.441846627196646 - bias: -2.7515445246676515 - loss: 0.4728430721517801)\n",
      "Epoch: 312/1000 - w1: 4.58287784678835 - w2: 1.4421231396100629 - bias: -2.7555142866949733 - loss: 0.4726441830446452)\n",
      "Epoch: 313/1000 - w1: 4.591988825911552 - w2: 1.4423994487964005 - bias: -2.759476750423661 - loss: 0.4724460461436974)\n",
      "Epoch: 314/1000 - w1: 4.601082473319831 - w2: 1.4426755663113053 - bias: -2.763431941723993 - loss: 0.47224865812965516)\n",
      "Epoch: 315/1000 - w1: 4.610158829224385 - w2: 1.4429515034405223 - bias: -2.7673798862336514 - loss: 0.47205201569974303)\n",
      "Epoch: 316/1000 - w1: 4.619217933785582 - w2: 1.4432272712049754 - bias: -2.7713206093612155 - loss: 0.47185611556759816)\n",
      "Epoch: 317/1000 - w1: 4.628259827111926 - w2: 1.4435028803657577 - bias: -2.7752541362896017 - loss: 0.4716609544631776)\n",
      "Epoch: 318/1000 - w1: 4.63728454925905 - w2: 1.44377834142903 - bias: -2.7791804919794383 - loss: 0.47146652913266657)\n",
      "Epoch: 319/1000 - w1: 4.646292140228726 - w2: 1.4440536646508326 - bias: -2.7830997011723855 - loss: 0.4712728363383867)\n",
      "Epoch: 320/1000 - w1: 4.655282639967903 - w2: 1.4443288600418103 - bias: -2.787011788394397 - loss: 0.4710798728587066)\n",
      "Epoch: 321/1000 - w1: 4.664256088367763 - w2: 1.4446039373718518 - bias: -2.7909167779589263 - loss: 0.4708876354879519)\n",
      "Epoch: 322/1000 - w1: 4.673212525262796 - w2: 1.4448789061746472 - bias: -2.7948146939700766 - loss: 0.470696121036316)\n",
      "Epoch: 323/1000 - w1: 4.682151990429907 - w2: 1.445153775752162 - bias: -2.7987055603256983 - loss: 0.4705053263297723)\n",
      "Epoch: 324/1000 - w1: 4.691074523587528 - w2: 1.445428555179031 - bias: -2.8025894007204317 - loss: 0.4703152482099871)\n",
      "Epoch: 325/1000 - w1: 4.6999801643947645 - w2: 1.4457032533068734 - bias: -2.8064662386487 - loss: 0.47012588353423135)\n",
      "Epoch: 326/1000 - w1: 4.708868952450553 - w2: 1.445977878768531 - bias: -2.8103360974076472 - loss: 0.4699372291752969)\n",
      "Epoch: 327/1000 - w1: 4.7177409272928434 - w2: 1.4462524399822285 - bias: -2.81419900010003 - loss: 0.46974928202140903)\n",
      "Epoch: 328/1000 - w1: 4.726596128397797 - w2: 1.4465269451556608 - bias: -2.818054969637057 - loss: 0.4695620389761421)\n",
      "Epoch: 329/1000 - w1: 4.735434595179007 - w2: 1.446801402290005 - bias: -2.82190402874118 - loss: 0.4693754969583367)\n",
      "Epoch: 330/1000 - w1: 4.744256366986734 - w2: 1.4470758191838622 - bias: -2.82574619994884 - loss: 0.46918965290201436)\n",
      "Epoch: 331/1000 - w1: 4.753061483107164 - w2: 1.4473502034371264 - bias: -2.8295815056131626 - loss: 0.469004503756296)\n",
      "Epoch: 332/1000 - w1: 4.76184998276168 - w2: 1.4476245624547852 - bias: -2.833409967906611 - loss: 0.46882004648531833)\n",
      "Epoch: 333/1000 - w1: 4.770621905106156 - w2: 1.4478989034506518 - bias: -2.8372316088235907 - loss: 0.46863627806815394)\n",
      "Epoch: 334/1000 - w1: 4.779377289230264 - w2: 1.448173233451029 - bias: -2.841046450183012 - loss: 0.4684531954987285)\n",
      "Epoch: 335/1000 - w1: 4.7881161741568015 - w2: 1.4484475592983093 - bias: -2.8448545136308083 - loss: 0.46827079578574116)\n",
      "Epoch: 336/1000 - w1: 4.796838598841035 - w2: 1.448721887654507 - bias: -2.848655820642411 - loss: 0.46808907595258414)\n",
      "Epoch: 337/1000 - w1: 4.805544602170057 - w2: 1.4489962250047304 - bias: -2.852450392525183 - loss: 0.467908033037264)\n",
      "Epoch: 338/1000 - w1: 4.814234222962166 - w2: 1.4492705776605888 - bias: -2.8562382504208115 - loss: 0.4677276640923221)\n",
      "Epoch: 339/1000 - w1: 4.822907499966253 - w2: 1.4495449517635393 - bias: -2.860019415307658 - loss: 0.4675479661847561)\n",
      "Epoch: 340/1000 - w1: 4.831564471861215 - w2: 1.4498193532881727 - bias: -2.8637939080030708 - loss: 0.46736893639594346)\n",
      "Epoch: 341/1000 - w1: 4.840205177255373 - w2: 1.4500937880454414 - bias: -2.8675617491656573 - loss: 0.4671905718215625)\n",
      "Epoch: 342/1000 - w1: 4.848829654685914 - w2: 1.4503682616858278 - bias: -2.8713229592975185 - loss: 0.46701286957151705)\n",
      "Epoch: 343/1000 - w1: 4.85743794261834 - w2: 1.4506427797024561 - bias: -2.875077558746444 - loss: 0.46683582676985935)\n",
      "Epoch: 344/1000 - w1: 4.866030079445939 - w2: 1.450917347434149 - bias: -2.8788255677080725 - loss: 0.4666594405547149)\n",
      "Epoch: 345/1000 - w1: 4.874606103489267 - w2: 1.4511919700684275 - bias: -2.882567006228014 - loss: 0.46648370807820677)\n",
      "Epoch: 346/1000 - w1: 4.883166052995643 - w2: 1.4514666526444582 - bias: -2.8863018942039362 - loss: 0.4663086265063812)\n",
      "Epoch: 347/1000 - w1: 4.891709966138657 - w2: 1.4517414000559465 - bias: -2.8900302513876173 - loss: 0.466134193019133)\n",
      "Epoch: 348/1000 - w1: 4.900237881017698 - w2: 1.4520162170539788 - bias: -2.893752097386962 - loss: 0.4659604048101324)\n",
      "Epoch: 349/1000 - w1: 4.9087498356574875 - w2: 1.4522911082498116 - bias: -2.8974674516679864 - loss: 0.4657872590867513)\n",
      "Epoch: 350/1000 - w1: 4.917245868007634 - w2: 1.4525660781176122 - bias: -2.901176333556766 - loss: 0.46561475306999006)\n",
      "Epoch: 351/1000 - w1: 4.925726015942192 - w2: 1.4528411309971487 - bias: -2.9048787622413546 - loss: 0.4654428839944057)\n",
      "Epoch: 352/1000 - w1: 4.9341903172592385 - w2: 1.453116271096432 - bias: -2.9085747567736684 - loss: 0.4652716491080398)\n",
      "Epoch: 353/1000 - w1: 4.942638809680464 - w2: 1.45339150249431 - bias: -2.91226433607134 - loss: 0.46510104567234706)\n",
      "Epoch: 354/1000 - w1: 4.95107153085077 - w2: 1.453666829143015 - bias: -2.9159475189195407 - loss: 0.4649310709621242)\n",
      "Epoch: 355/1000 - w1: 4.959488518337886 - w2: 1.453942254870665 - bias: -2.919624323972772 - loss: 0.46476172226543927)\n",
      "Epoch: 356/1000 - w1: 4.967889809631988 - w2: 1.4542177833837202 - bias: -2.9232947697566276 - loss: 0.4645929968835612)\n",
      "Epoch: 357/1000 - w1: 4.976275442145338 - w2: 1.4544934182693938 - bias: -2.9269588746695274 - loss: 0.46442489213089116)\n",
      "Epoch: 358/1000 - w1: 4.984645453211936 - w2: 1.4547691629980217 - bias: -2.93061665698442 - loss: 0.464257405334892)\n",
      "Epoch: 359/1000 - w1: 4.9929998800871696 - w2: 1.4550450209253871 - bias: -2.934268134850457 - loss: 0.46409053383601956)\n",
      "Epoch: 360/1000 - w1: 5.001338759947489 - w2: 1.4553209952950035 - bias: -2.9379133262946424 - loss: 0.46392427498765465)\n",
      "Epoch: 361/1000 - w1: 5.00966212989009 - w2: 1.4555970892403576 - bias: -2.9415522492234514 - loss: 0.46375862615603475)\n",
      "Epoch: 362/1000 - w1: 5.0179700269326 - w2: 1.45587330578711 - bias: -2.9451849214244237 - loss: 0.46359358472018636)\n",
      "Epoch: 363/1000 - w1: 5.026262488012782 - w2: 1.456149647855258 - bias: -2.94881136056773 - loss: 0.4634291480718573)\n",
      "Epoch: 364/1000 - w1: 5.0345395499882475 - w2: 1.456426118261257 - bias: -2.952431584207713 - loss: 0.46326531361545026)\n",
      "Epoch: 365/1000 - w1: 5.042801249636176 - w2: 1.4567027197201048 - bias: -2.956045609784402 - loss: 0.46310207876795667)\n",
      "Epoch: 366/1000 - w1: 5.051047623653049 - w2: 1.4569794548473887 - bias: -2.9596534546250037 - loss: 0.46293944095888917)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5.051047623653049, 1.4569794548473887, -2.9596534546250037)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "def gradient_descent(age, affordibility, y_actual, epochs, loss_threshold):\n",
    "    w1 = w2 = 1\n",
    "    bias, rate, n = 0, 0.5, len(age)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        weighted_sum = w1 * age + w2 * affordibility + bias\n",
    "\n",
    "        # predict and calculate loss\n",
    "        y_predicted = sigmoid(weighted_sum)\n",
    "        loss = log_loss(y_actual, y_predicted)\n",
    "\n",
    "        w1d = (1/n) * np.dot(np.transpose(age), (y_predicted - y_actual))\n",
    "        w2d = (1/n) * np.dot(np.transpose(affordibility), (y_predicted - y_actual))\n",
    "\n",
    "        bias_d = np.mean(y_predicted - y_actual)\n",
    "\n",
    "        w1 = w1 - rate * w1d\n",
    "        w2 = w2 - rate * w2d\n",
    "        bias = bias - rate * bias_d\n",
    "        print(f\"Epoch: {i}/{epochs} - w1: {w1} - w2: {w2} - bias: {bias} - loss: {loss})\")\n",
    "\n",
    "        if loss <= loss_threshold:\n",
    "            break\n",
    "\n",
    "    return w1, w2, bias\n",
    "\n",
    "gradient_descent(X_train_scaled[\"age\"], X_train_scaled[\"affordibility\"], y_train, 1000, 0.4631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[5.0608673],\n",
       "        [1.4086505]], dtype=float32),\n",
       " array([-2.9137032], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "coef, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}